# -*- coding: utf-8 -*-
"""4580_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WYTpIXn4KaKs92LFdmhaTQK1PsfHI5Rf

The following test block contains code to run a simulation of Queries being processed in an LLM model. There is baseline model, a first-come, first-serve model, and an improved model, decode-priortizing with batching.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from dataclasses import dataclass, field
from collections import deque
from typing import List, Optional

#Default Parameters for Simulation
DEFAULT_C = 35 #setup time
DEFAULT_A = 0.30 #time per a token
DEFAULT_B0 = 64 #minimum batch size
DEFAULT_K = 128 #maximum batch size. number of jobs

#Request are Queries entering the System
@dataclass
class Request:
    req_id: int
    arrival_time: float
    prompt_len: int
    output_len: int

    tokens_generated: int = 0
    start_time: Optional[float] = None
    first_token_time: Optional[float] = None
    end_time: Optional[float] = None

    @property
    def is_prefill_complete(self):
        return self.tokens_generated > 0

    @property
    def is_finished(self):
        return self.tokens_generated >= self.output_len

#Tracks Time and Number of Queries in System
class SystemStateTracker:
    def __init__(self):
        self.timestamps = []
        self.queries_in_system_counts = []

    def record_state(self, current_time: float, queries_in_system: int):
        self.timestamps.append(current_time)
        self.queries_in_system_counts.append(queries_in_system)

#Serves Request through inputted server parameters and batching/scheduling parameters
class LLMEngine:
    def __init__(self, c=DEFAULT_C, a=DEFAULT_A, b0=DEFAULT_B0, max_batch_size=DEFAULT_K, mode="batch", numgpus = 1):
        self.c = c
        self.a = a
        self.b0 = b0
        self.max_batch_size = max_batch_size
        self.mode=mode

        self.num_gpus = numgpus
        self.waiting_queue = deque()
        self.running_queue = []
        self.completed_requests = []
        self.current_time = 0.0
        self.state_tracker = SystemStateTracker()

        self.completion_times = []

    def add_request(self, req: Request):
        self.waiting_queue.append(req)

    def get_batch_service_time(self, batch_tokens: int) -> float:
        if batch_tokens == 0:
            return 0.0

        #ms_time = self.c + self.a * max(0, batch_tokens - self.b0)
        ms_time = self.c + self.a * max(0, batch_tokens - self.b0) + 0.02 * (batch_tokens ** 1.5)
        ms_time = ms_time/self.num_gpus
        return ms_time / 1000.0

    def step_fcfs(self):
      self.state_tracker.record_state(
        self.current_time,
        len(self.waiting_queue) + len(self.running_queue)
      )

      if self.running_queue:
          req = self.running_queue[0]
      else:

          if not self.waiting_queue:
              return False
          req = self.waiting_queue.popleft()
          self.running_queue.append(req)
          req.start_time = self.current_time


      if not req.is_prefill_complete:
          batch_tokens = req.prompt_len
          dt = self.get_batch_service_time(batch_tokens)
          self.current_time += dt
          req.tokens_generated = 1
          req.first_token_time = self.current_time
      else:
          batch_tokens = 1
          dt = self.get_batch_service_time(1)
          self.current_time += dt
          req.tokens_generated += 1

      if req.is_finished:
          req.end_time = self.current_time
          self.completed_requests.append(req)
          self.completion_times.append(req.end_time)
          self.running_queue = []

      return True

    def step_batching(self):

        self.state_tracker.record_state(self.current_time, len(self.waiting_queue) + len(self.running_queue))

        current_batch_tokens = 0
        batch_requests = []

        active_decodes = []
        for req in self.running_queue:
            if  not len(batch_requests) or len(batch_requests) + 1 <= self.max_batch_size:
                current_batch_tokens += 1
                batch_requests.append((req, "decode"))
                active_decodes.append(req)
            else:
                break

        if self.waiting_queue:
            candidate = self.waiting_queue[0]
            if not len(batch_requests) or len(batch_requests) + 1 <= self.max_batch_size:
                req = self.waiting_queue.popleft()
                current_batch_tokens += req.prompt_len
                batch_requests.append((req, "prefill"))
            else:
                pass

        if current_batch_tokens == 0:
            return False

        dt = self.get_batch_service_time(current_batch_tokens)

        self.current_time += dt

        for req, phase in batch_requests:
            if phase == "prefill":
                req.start_time = self.current_time - dt
                req.tokens_generated += 1
                req.first_token_time = self.current_time
                self.running_queue.append(req)

            elif phase == "decode":
                req.tokens_generated += 1
                if req.is_finished:
                    req.end_time = self.current_time
                    self.completed_requests.append(req)

        self.running_queue = [r for r in self.running_queue if not r.is_finished]

        return True

    def step(self):
        if self.mode == "fcfs":
            return self.step_fcfs()
        else:
            return self.step_batching()



def run_simulation(arrival_rate=0.25, total_queries=1000, seed=None, verbose=True, warmup_time=0, analysis_time=float('inf'), mode = "fcfs", num_gpus=1):
    np.random.seed(seed)

    engine = LLMEngine(mode=mode, numgpus = num_gpus)

    inter_arrivals = np.random.exponential(1.0/arrival_rate, total_queries)
    arrival_times = np.cumsum(inter_arrivals)

    requests = []
    for i in range(total_queries):
        l_i = np.random.choice([64, 128, 512], p=[0.5, 0.3, 0.2])
        b_i = min(32, np.random.geometric(p=1/16))
        b_i = max(1, b_i)

        req = Request(req_id=i, arrival_time=arrival_times[i], prompt_len=l_i, output_len=b_i)
        requests.append(req)

    pending_arrivals = deque(requests)

    while pending_arrivals or engine.waiting_queue or engine.running_queue:

        while pending_arrivals and pending_arrivals[0].arrival_time <= engine.current_time:
            engine.add_request(pending_arrivals.popleft())
        is_busy = engine.step()

        if not is_busy:

            if pending_arrivals:
                next_arrival = pending_arrivals[0].arrival_time

                engine.current_time = max(engine.current_time, next_arrival)
            else:

                break

    timestamps = np.array(engine.state_tracker.timestamps)
    queries_in_system_counts = np.array(engine.state_tracker.queries_in_system_counts)

    filter_mask = (timestamps >= warmup_time) & (timestamps <= analysis_time)
    filtered_timestamps = timestamps[filter_mask]
    filtered_counts = queries_in_system_counts[filter_mask]

    if len(filtered_timestamps) < 2:
        if verbose:
            print("Not enough data points after filtering for time-weighted average calculation.")
        return 0.0

    total_weighted_sum = 0.0
    total_duration = 0.0

    for i in range(len(filtered_timestamps) - 1):
        duration = filtered_timestamps[i+1] - filtered_timestamps[i]
        if duration > 0:
            total_weighted_sum += filtered_counts[i] * duration
            total_duration += duration

    if total_duration == 0:
        time_weighted_avg_queries = 0.0
    else:
        time_weighted_avg_queries = total_weighted_sum / total_duration

    throughput = len(engine.completed_requests) / engine.current_time

    ttft_list = []
    tbt_list = []

    for req in engine.completed_requests:
        if req.first_token_time is not None and req.start_time is not None:
            ttft = req.first_token_time - req.arrival_time
            ttft_list.append(ttft)

        if req.end_time is not None and req.first_token_time is not None:
            if req.output_len > 1:
                tbt = (req.end_time - req.first_token_time) / (req.output_len - 1)
                tbt_list.append(tbt)

    avg_ttft = np.mean(ttft_list) if ttft_list else 0.0
    ttft95 = np.percentile(ttft_list, 95) if ttft_list else 0.0
    avg_tbt = np.mean(tbt_list) if tbt_list else 0.0
    tbt95 = np.percentile(tbt_list, 95) if tbt_list else 0.0
    avg_ttft = np.mean(ttft_list) if ttft_list else 0.0
    avg_tbt = np.mean(tbt_list) if tbt_list else 0.0

    if verbose:
        print(f"Simulation Complete at time {engine.current_time:.2f}")
        print(f"Throughput: {throughput:.2f}")
        print(f"Avg TTFT: {avg_ttft:.4f} s")
        print(f"Avg TBT: {avg_tbt:.4f} s")
        print(f"95th Percentile TTFT: {ttft95:.4f} s")
        print(f"95th Percentile TBT: {tbt95:.4f} s")

    return time_weighted_avg_queries, throughput, avg_tbt, tbt95, avg_ttft, ttft95

"""Here, the first-come, first-serve simulation is run for a range of arrival rates from 0.05 to 5. Outputting key metrics of performance: average queries in system, throughput (queries per second), time between tokens (TBT), 95th percentile TBT (tail latency), time to first token (TTFT), 95th percentile TFFT (tail latency)."""

#fcfs main simulation

if __name__ == "__main__":
    print("Baseline Simulation")
    avg_queries_baseline, _, _, _, _, _ = run_simulation(
        arrival_rate=0.25,
        total_queries=10000,
        verbose=True,
        warmup_time=5.0,
        analysis_time=100.0
    )
arrival_rates_lambda = np.arange(0.05, 5.0, 0.8)

results_data = []

for lambda_val in arrival_rates_lambda:
    avg_queries, throughput, tbt, tbt95, tfft, tfft95 = run_simulation(
          arrival_rate=lambda_val,
          total_queries=10000,
          verbose=False,
          warmup_time=5.0,
          analysis_time=100.0,
        )
    results_data.append({
        "arrival_rate": lambda_val,
        "avg_queries_in_system": avg_queries,
        "throughput": throughput,
        "tbt": tbt,
        "tfft": tfft,
        "tbt95": tbt95,
        "tfft95": tfft95
        })

lambda_vs_queries_df = pd.DataFrame(results_data)
print("\nResults DataFrame head:")
print(lambda_vs_queries_df.head())

plt.figure(figsize=(10, 6))
plt.plot(lambda_vs_queries_df['arrival_rate'], lambda_vs_queries_df['avg_queries_in_system'], marker='o', linestyle='-')
plt.title('Average Queries in System vs. Arrival Rate (\u03bb) for First Come First Serve Ordering')
plt.xlabel('Arrival Rate (\u03bb) (queries/second)')
plt.ylabel('Average Queries in System (L)')
plt.grid(True)
plt.show()

"""Plot the confidence interval of the baseline simulation for arrival rate vs. average queries in the system."""

import numpy as np
import matplotlib.pyplot as plt

def compute_ci(arr, confidence=0.95):
    arr = np.array(arr)
    mean = np.mean(arr)
    std = np.std(arr, ddof=1)
    n = len(arr)
    margin = 1.96 * std / np.sqrt(n)
    return mean, mean - margin, mean + margin

N_REPS = 20   # number of repeats per λ
TOTAL_Q = 4000

lambda_vals = np.arange(0.05, 5.0, 0.8)

mean_L_list = []
ci_low_list = []
ci_high_list = []

print("Running CI analysis across arrival rates...")

for lam in lambda_vals:
    L_values = []

    for rep in range(N_REPS):
        avg_queries, throughput, avg_tbt, tbt95, avg_ttft, ttft95 = run_simulation(
            arrival_rate=lam,
            total_queries=TOTAL_Q,
            verbose=False,
            warmup_time=5.0,
            analysis_time=100.0,
            mode="fcfs"
        )
        L_values.append(avg_queries)

    # compute CI for this arrival rate
    mean_L, lower, upper = compute_ci(L_values)

    mean_L_list.append(mean_L)
    ci_low_list.append(lower)
    ci_high_list.append(upper)

    print(f"λ={lam:.2f}: L={mean_L:.3f}, CI=({lower:.3f}, {upper:.3f})")


plt.figure(figsize=(10, 6))

# Plot mean line with markers
plt.plot(lambda_vals, mean_L_list, marker='o', linestyle='-', label='Mean L (Avg Queries in System)')

# Add shaded 95% CI band
plt.fill_between(lambda_vals, ci_low_list, ci_high_list, alpha=0.25, label="95% CI")

# Add vertical lines (error bars) at each point
plt.errorbar(lambda_vals, mean_L_list,
             yerr=[np.array(mean_L_list)-np.array(ci_low_list), np.array(ci_high_list)-np.array(mean_L_list)],
             fmt='o', color='black', capsize=5, linestyle='None')

plt.title("Baseline: Average Queries in System vs Arrival Rate (with 95% CI)")
plt.xlabel("Arrival Rate λ")
plt.ylabel("Avg Queries in System (L)")
plt.grid(True)
plt.legend()
plt.show()

"""Validation Helper: Compare Baseline Simulation to M/M/1 Queue"""

def validate_mm1():
    print("\n--- Running Validation (Approximating M/M/1) ---")
    print("Note: Our simulator uses deterministic service times, M/M/1 requires exponential. This is an approximation.")

    avg_queries, throughput, tbt, tbt95, tfft, tfft95 = run_simulation(
        arrival_rate=0.1,
        total_queries=2000,
        verbose=True,
        warmup_time=0,
        analysis_time=float('inf')
    )

validate_mm1()

"""Baseline Simulation throughput steadies at about 1.7 queries/second once arrival rate reaches 1.7 queries/second."""

'''Throughput vs. Arrival Rate Plot'''

plt.figure()
plt.plot(lambda_vs_queries_df['arrival_rate'], lambda_vs_queries_df['throughput'], marker='o')
plt.xlabel("Arrival rate λ")
plt.ylabel("Throughput (req/sec)")
plt.title("Baseline: Throughput vs Arrival Rate")
plt.grid(True)
plt.show()

"""Applies batching to see average queries in the system rise later than in the baseline simulation."""

if __name__ == "__main__":
    print("Baseline Simulation - Batching")
    avg_queries_baseline = run_simulation(
        arrival_rate=0.25,
        total_queries=10000,
        verbose=True,
        warmup_time=5.0,
        analysis_time=100.0,
        mode="batch"
    )
arrival_rates_lambda = np.arange(0.05, 15, 0.5)

results_data = []

for lambda_val in arrival_rates_lambda:
    avg_queries, throughput, tbt, tbt95, tfft, tfft95 = run_simulation(
          arrival_rate=lambda_val,
          total_queries=10000,
          verbose=False,
          warmup_time=5.0,
          analysis_time=100.0,
          mode="batch"
        )
    results_data.append({
        "arrival_rate": lambda_val,
        "avg_queries_in_system": avg_queries,
        "throughput": throughput
        })

lambda_vs_queries_df = pd.DataFrame(results_data)
print("\nResults DataFrame head:")
print(lambda_vs_queries_df)

plt.figure(figsize=(10, 6))
plt.plot(lambda_vs_queries_df['arrival_rate'], lambda_vs_queries_df['avg_queries_in_system'], marker='o', linestyle='-')
plt.title('Average Queries in System vs. Arrival Rate (\u03bb) with Improved Batching')
plt.xlabel('Arrival Rate (\u03bb) (queries/second)')
plt.ylabel('Average Queries in System (L)')
plt.grid(True)
plt.show()

"""With batching throughput steadies at just under 7 queries per a second, far higher than observed with the baseline."""

'''Throughput vs. Arrival Rate Plot for Mode: batch'''

plt.figure()
plt.plot(lambda_vs_queries_df['arrival_rate'], lambda_vs_queries_df['throughput'], marker='o')
plt.xlabel("Arrival rate λ")
plt.ylabel("Throughput (req/sec)")
plt.title("Throughput vs Arrival Rate with Improved Batching")
plt.grid(True)
plt.show()

"""Confidence intervals of average queries in the system across arrival times when varying number of GPUs. Average queries in the system rise more quickly with less GPUs."""

if __name__ == "__main__":
    import matplotlib.pyplot as plt

    lambda_vals = np.arange(0.05, 30, 4)
    gpu_counts = [1, 2, 4]
    N_REPS = 5
    TOTAL_Q = 4000

    def compute_ci(arr, confidence=0.95):
        arr = np.array(arr)
        mean = np.mean(arr)
        std = np.std(arr, ddof=1)
        n = len(arr)
        margin = 1.96 * std / np.sqrt(n)
        return mean, mean - margin, mean + margin

    results = []

    for num_gpus in gpu_counts:
        for lam in lambda_vals:
            L_values = []
            T_values  = []
            for rep in range(N_REPS):
                avg_queries, throughput, _, _, _, _ = run_simulation(
                    arrival_rate=lam,
                    total_queries=TOTAL_Q,
                    verbose=False,
                    warmup_time=5.0,
                    analysis_time=100.0,
                    mode="batch",
                    num_gpus=num_gpus
                )
                L_values.append(avg_queries)
            mean_L, lower, upper = compute_ci(L_values)
            results.append({
                "num_gpus": num_gpus,
                "lambda": lam,
                "mean_L": mean_L,
                "ci_low": lower,
                "ci_high": upper
            })

    df = pd.DataFrame(results)

    # Plotting
    plt.figure(figsize=(10,6))
    for num_gpus in gpu_counts:
        subset = df[df['num_gpus']==num_gpus]
        plt.plot(subset['lambda'], subset['mean_L'], marker='o', label=f"{num_gpus} GPU(s)")
        plt.fill_between(subset['lambda'], subset['ci_low'], subset['ci_high'], alpha=0.25)
    plt.xlabel("Arrival Rate λ")
    plt.ylabel("Avg Queries in System (L)")
    plt.title("Effect of GPU Scaling on Avg Queries in System (with 95% CI)")
    plt.grid(True)
    plt.legend()
    plt.show()

"""Measure throughput and tail latency for Batching to compare to Chunked Prefill."""

_, throughput, _, p95_tbt, _, _ = run_simulation(
    arrival_rate=10,
    total_queries=10000,
    verbose=False,
    warmup_time=5.0,
    analysis_time=100.0,
    mode="batch"
)

print(f"For batching mode with arrival rate 10:")
print(f"  Throughput: {throughput:.2f} req/s")
print(f"  P95 TBT: {p95_tbt:.4f} s")

"""Chunked Prefill affects tail latency and throughput differently across chunk sizes. The performance of batching can be beaten through chunked prefill, particularly when an optimal chunk size is chosen."""

# Chunked Prefill Implementation & Analysis

class ChunkedLLMEngine(LLMEngine):
    """
    Extends the base LLMEngine to support Chunked Prefills.
    The prefill phase of a request can be split into multiple batches.
    """
    def __init__(self, chunk_size=None, **kwargs):
        super().__init__(**kwargs)
        self.chunk_size = chunk_size if chunk_size is not None else float('inf')

        self.prefill_progress = {}

    def step_batching(self):
        """
        Overrides the standard batching step to allow partial prefill processing.
        """
        self.state_tracker.record_state(
            self.current_time,
            len(self.waiting_queue) + len(self.running_queue)
        )

        current_batch_tokens = 0

        batch_actions = []

        for req in self.running_queue:
            if not len(batch_actions) or len(batch_actions) + 1 <= self.max_batch_size:
                current_batch_tokens += 1
                batch_actions.append((req, "decode", 1))
            else:
                break # when batch is full

        if self.waiting_queue:
            req = self.waiting_queue[0]

            processed = self.prefill_progress.get(req.req_id, 0)
            remaining_prompt = req.prompt_len - processed

            if not len(batch_actions):
              tokens_to_process = min(remaining_prompt, self.chunk_size)
            elif len(batch_actions) < self.max_batch_size:
              tokens_to_process = min(remaining_prompt, self.chunk_size)
            else:
              tokens_to_process = 0

            if tokens_to_process > 0:
                current_batch_tokens += tokens_to_process
                batch_actions.append((req, "prefill", tokens_to_process))

        if current_batch_tokens == 0:
            return False

        dt = self.get_batch_service_time(current_batch_tokens)
        self.current_time += dt

        for req, phase, token_count in batch_actions:
            if phase == "decode":
                req.tokens_generated += 1
                if req.is_finished:
                    req.end_time = self.current_time
                    self.completed_requests.append(req)

            elif phase == "prefill":
                curr = self.prefill_progress.get(req.req_id, 0)
                new_progress = curr + token_count
                self.prefill_progress[req.req_id] = new_progress

                if req.start_time is None:
                    req.start_time = self.current_time - dt

                # checking if prefill is complete
                if new_progress >= req.prompt_len:
                    if self.waiting_queue and self.waiting_queue[0] == req:
                        self.waiting_queue.popleft()

                    del self.prefill_progress[req.req_id]

                    req.tokens_generated = 1
                    req.first_token_time = self.current_time
                    self.running_queue.append(req)

        self.running_queue = [r for r in self.running_queue if not r.is_finished]

        return True

def run_chunk_experiment(chunk_size, arrival_rate=1.5, total_queries=1000, seed=42):
    np.random.seed(seed)

    engine = ChunkedLLMEngine(chunk_size=chunk_size, c=DEFAULT_C, a=DEFAULT_A, b0=DEFAULT_B0, max_batch_size=DEFAULT_K)

    inter_arrivals = np.random.exponential(1.0/arrival_rate, total_queries)
    arrival_times = np.cumsum(inter_arrivals)

    requests = []
    for i in range(total_queries):
        l_i = np.random.choice([64, 128, 512], p=[0.5, 0.3, 0.2])
        b_i = max(1, min(32, np.random.geometric(p=1/16)))
        req = Request(req_id=i, arrival_time=arrival_times[i], prompt_len=l_i, output_len=b_i)
        requests.append(req)

    pending_arrivals = deque(requests)

    while pending_arrivals or engine.waiting_queue or engine.running_queue:
        while pending_arrivals and pending_arrivals[0].arrival_time <= engine.current_time:
            engine.add_request(pending_arrivals.popleft())

        is_busy = engine.step_batching() # chunked step method

        if not is_busy:
            if pending_arrivals:
                engine.current_time = max(engine.current_time, pending_arrivals[0].arrival_time)
            else:
                break

    tbt_list = []
    ttft_list = []

    for req in engine.completed_requests:
        if req.first_token_time and req.arrival_time:
            ttft_list.append(req.first_token_time - req.arrival_time)
        if req.end_time and req.first_token_time and req.output_len > 1:
            tbt_list.append((req.end_time - req.first_token_time) / (req.output_len - 1))

    throughput = len(engine.completed_requests) / engine.current_time
    p95_tbt = np.percentile(tbt_list, 95) if tbt_list else 0
    p95_ttft = np.percentile(ttft_list, 95) if ttft_list else 0

    return throughput, p95_tbt, p95_ttft

FIXED_LAMBDA = 10
CHUNK_SIZES = [32, 64, 128, 256, 512, 1024, 2048]

print(f"--- Running Chunked Prefill Sensitivity Study (Lambda={FIXED_LAMBDA}) ---")
print(f"{'Chunk Size':<12} | {'Throughput':<12} | {'P95 TBT (s)':<12} | {'P95 TTFT (s)':<12}")
print("-" * 60)

results_chunking = []

for size in CHUNK_SIZES:
    tp, tbt, ttft = run_chunk_experiment(chunk_size=size, arrival_rate=FIXED_LAMBDA, total_queries=2000)
    results_chunking.append({
        "chunk_size": size,
        "throughput": tp,
        "p95_tbt": tbt,
        "p95_ttft": ttft
    })
    print(f"{size:<12} | {tp:<12.2f} | {tbt:<12.4f} | {ttft:<12.4f}")

# visualization
df_chunk = pd.DataFrame(results_chunking)

fig, ax1 = plt.subplots(figsize=(10, 6))

color = 'tab:red'
ax1.set_xlabel('Chunk Size (Tokens)')
ax1.set_ylabel('P95 Time Between Tokens (s)', color=color)
ax1.plot(df_chunk['chunk_size'], df_chunk['p95_tbt'], marker='o', color=color, label='P95 TBT (Chunking)')
ax1.tick_params(axis='y', labelcolor=color)
ax1.set_xscale('log', base=2)
ax1.set_xticks(CHUNK_SIZES)
ax1.get_xaxis().set_major_formatter(plt.ScalarFormatter())
ax1.grid(True, which="both", ls="-", alpha=0.3)

ax2 = ax1.twinx()
color = 'tab:blue'
ax2.set_ylabel('Throughput (req/s)', color=color)
ax2.plot(df_chunk['chunk_size'], df_chunk['throughput'], marker='x', linestyle='--', color=color, label='Throughput (Chunking)')
ax2.tick_params(axis='y', labelcolor=color)

benchmark_throughput = 6.85
benchmark_p95_tbt = 0.2355

ax1.axhline(y=benchmark_p95_tbt, color='red', linestyle=':', label='P95 TBT (Batching, no chunking)')
ax2.axhline(y=benchmark_throughput, color='blue', linestyle=':', label='Throughput (Batching, no chunking)')

ax1.legend(loc='upper left')
ax2.legend(loc='upper right')

plt.title(f'Effect of Chunk Size on Latency (TBT) and Throughput ($\lambda$={FIXED_LAMBDA})')
plt.show()

print("\n--- Analysis ---")
print("1. Impact on TBT (Stalls): Smaller chunks significantly reduce P95 Time Between Tokens (TBT).")
print("   This is because large prefills (e.g., 512 tokens) no longer block decode iterations for the full duration.")
print("   Decode steps can sneak in between chunks of a large prefill.")
print("2. Trade-off (Throughput): Very small chunks (e.g., 32) might slightly reduce throughput.")
print("   This is due to the fixed overhead 'c' in the service time model S(b) = c + a*b.")
print("   Splitting one large batch into many small ones incurs the fixed cost 'c' multiple times.")
print("3. Recommendation: A chunk size around 64-128 appears optimal here, balancing low TBT with minimal throughput loss.")